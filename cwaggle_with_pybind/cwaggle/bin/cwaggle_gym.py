
# this is purely to hide some annoying tensorflow warnings.
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

import control_bridge as cb
import numpy as np
import gym
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam
from rl.agents import DQNAgent
from rl.policy import BoltzmannQPolicy
from rl.memory import SequentialMemory

#WIP

class cwaggle_gym(gym.Env):
    def __init__(self):
        # inital setup of the environment and world.
        #controlLine = cb.PybindControl()
        self.controlLine = cb.PybindControl()
        # < see which one is better.
        # > self. is needed to ensure that the same object is called 
        # throughout the class.
        # The action space is the actions we can take. on the c++ side
        # these actions are move stright, turn left a little, turn left 
        # a lot, turn right a little, and turn right a lot. On the python
        # side these actions will be reperesented as 0, 1, 2, 3, 4, 
        # respectivly
        self.action_space = gym.spaces.Discrete(5)
        self.observation_space = gym.spaces.Discrete(8)
        # < figure out how to tie in the observation space.
        # > there are 8 possible states, so make it discrete(8)
        #000 ; 0 -> all sensors clear
        #001 ; 1 -> left sensor activated
        #010 ; 2 -> right sensor activated
        #100 ; 3 -> front sensor activated
        #011 ; 4 -> right and left sensors activated
        #110 ; 5 -> front and right sensors activated
        #101 ; 6 -> front and left sensors activated
        #111 ; 7 -> all snesors activated
        self.state = 0 #self.controlLine.getSensorReadings(self.controlLine.getRobot())
        # < figure out how to tie in the environments state. would it be 
        # the sensor readings of robots? Refer to stricklands paper for
        # sensor reading to state represnetation. Should the state be an array 
        # of readings, consisting of one reading per robot?
        # > state is the sensor readings of the robot(s)
        # will need to be modified for multi-agent handeling, also needs to be changed to 
        # match with discrete space representation.
        self.simulation_length = 1000
        # < figure out what to use to describe the length used for the simulation.

    # it is expected that 'actions' will be an array of actions to take corrosponding
    # to the number of robots there are in the environment
    # << this si for multiagent handeling. when doing single agent trils, 'actions' will
    # be a single integer.
    def step(self, actions):
        # < add 'actions' to input field?
        # > yes, have a list of actions compiled before invoking step, based on the 
        # data stored in the state space. If formatted correctly, then the actions space
        # will be built and called upon such that the actions generated by each state will
        # have a mathcing index in their own arrays, leading to the correct robots getting 
        # the correct actions. << This is setup for multiagent controls, can be ignored
        # when working with single agents.
        #for i in range(self.controlLine.numberOfRobots()):
           #self.controlLine.takeAction(self.controlLine.getRobot(), actions[i])
           #self.controlLine.nextRobot()
        # << This one is for single agent ahndeling
        self.controlLine.takeAction(self.controlLine.getRobot(), actions)
        
        # update the state with the robots new sensor readings
        readings = self.controlLine.getSensorReadings(self.controlLine.getRobot())
        # see self.observation_space comments for the meaning of different readings
        if readings[0] == 0 and readings[1] == 0 and readings[2] == 0:
            self.state = 0
        elif readings[0] == 0 and readings[1] == 0 and readings[2] >= 1:
            self.state = 1
        elif readings[0] == 0 and readings[1] >= 1 and readings[2] == 0:
            self.state = 2
        elif readings[0] >= 1 and readings[1] == 0 and readings[2] == 0:
            self.state = 3
        elif readings[0] == 0 and readings[1] >= 1 and readings[2] >= 1:
            self.state = 4
        elif readings[0] >= 1 and readings[1] >= 1 and readings[2] == 0:
            self.state = 5
        elif readings[0] >= 1 and readings[1] == 0 and readings[2] >= 1:
            self.state = 6
        elif readings[0] >= 1 and readings[1] >= 1 and readings[2] >= 1:
            self.state = 7

        # step the simulation environment forward
        self.controlLine.simulatorUpdate()
        
        #also include other aspects of running the simulation?
        #actions take place here? need to make sure.
        # < do i need to build a while loop here?
        # > no, while loop is not needed here. the step really is just one
        # simulation step. actions take place at start
        
        # decrement the simulation length counter
        self.simulation_length -= 1
        # check and see if the simulation episode is over.
        if self.simulation_length <= 0:
           done = True
        else:
           done = False

        #reward function is based on the number of collisions made up to this point
        reward = -self.controlLine.getCollisions(self.controlLine.getRobot())
        # collision count needs to be reset so that we don't get negative rewards for 
        # neutral and good actions.
        self.controlLine.resetCollisions(self.controlLine.getRobot())
        # < see if this needs to be reset

        #set placeholder for info, necessary for OpenAIGym structure
        info = {}

        return self.state, reward, done, info
        

    def render(self):
        # setup and run the GUI.
        #controlLine.makeGUI()
        #self.controlLine.makeGUI()
        # < needs to be modified so that the GUI is only made on the inital call
        # maybe make some sort of function to check if the GUI is made?
        # >
        if self.controlLine.isGUI() == False:
           self.controlLine.makeGUI()
        
        # update the GUI to follow the simulation
        self.controlLine.updateGUI()
        pass

    def reset(self):
        #reinitialize/reset the environment
        self.controlLine = cb.PybindControl()
        self.state = 0
        self.simulation_length = 1000

def buildModel(states, actions):
    model = Sequential()
    model.add(Dense(24, activation='relu', input_shape=states))
    model.add(Dense(24, activation='relu'))
    model.add(Dense(actions, activation='linear'))
    return model

def buildAgent(model, actions):
    policy = BoltzmannQPolicy()
    memory = SequentialMemory(limit='50000', window_length=1)
    dqn = DQNAgent(model=model, memory=memory, policy=policy, nb_action=actions, nb_steps_warmup=10, target_model_update=1e-2)
    return dqn

# a quick test to make sure that the gym environment runs as it should
# with a simple random action case.
def playTest():
    env = cwaggle_gym()
    #print(env.action_space.sample())
    #test case with random actions
    episodes = 3
    for episode in range(1, episodes+1):
        env.reset()
        done = False
        score = 0

        while not done:
            env.render()
            action = env.action_space.sample()
            n_state, reward, done, info = env.step(action)
            score += reward
            # < see if accumulation needs to be ommited.
        print("Episode: {} Score: {}".format(episode, score))

def modelTest():
    env = cwaggle_gym()
    #states = env.observation_space
    states = (1,)
    actions = env.action_space.n
    #print(states)
    model = buildModel(states, actions)
    model.summary()



def main():
    playTest() 
    #modelTest()
    print("Main done")


main()